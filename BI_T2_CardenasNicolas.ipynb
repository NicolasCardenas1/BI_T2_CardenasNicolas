{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6d70fe9",
   "metadata": {},
   "source": [
    "# Trabajo N.º 2 \n",
    "## Modelamiento de patrones sísmicos en Chile a través de regresión, clasificación y clustering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fbc1bb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cd9e1ec",
   "metadata": {},
   "source": [
    "###  Abstract\n",
    "\n",
    "Este trabajo analiza la **actividad sísmica en Chile (2012–2025)** con el objetivo de aplicar técnicas de **regresión, clasificación y agrupamiento** para responder hipótesis de Inteligencia de Negocios (BI).\n",
    "\n",
    "Se utiliza un dataset de sismos del **Centro Sismológico Nacional (CSN)**, que contiene variables como magnitud, profundidad, latitud, longitud y fecha.  \n",
    "El análisis comienza con la limpieza y estandarización de los datos, seguido por una **exploración descriptiva y correlacional** que permite identificar las variables más influyentes sobre la magnitud de los eventos.\n",
    "\n",
    "Posteriormente, se desarrolla una **regresión** para explicar la magnitud según factores geográficos, una **clasificación** para distinguir eventos de alta magnitud, y un **agrupamiento (clustering)** para descubrir patrones espaciales en la actividad sísmica nacional.  \n",
    "Las métricas utilizadas incluyen **MAE, RMSE y R²** (para regresión), **accuracy, precisión, recall y F1-score** (para clasificación), y **silhouette e inercia** (para clustering).\n",
    "\n",
    "Finalmente, se discuten los resultados en términos de su utilidad para la toma de decisiones estratégicas en BI, identificando posibles zonas de riesgo y oportunidades para optimizar la gestión de alertas sísmicas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f35d8fe",
   "metadata": {},
   "source": [
    "###  Objetivos\n",
    "\n",
    "**Objetivo general:**  \n",
    "Analizar y modelar la actividad sísmica en Chile para identificar patrones relevantes que apoyen decisiones de Inteligencia de Negocios mediante técnicas de regresión, clasificación y agrupamiento.\n",
    "\n",
    "**Objetivos específicos:**\n",
    "1. Desarrollar una **exploración de datos (EDA)** con correlaciones, histogramas y visualizaciones temporales para comprender las relaciones entre magnitud, profundidad y ubicación.  \n",
    "2. Implementar un **modelo de regresión** que estime la magnitud en función de variables geográficas y de profundidad.  \n",
    "3. Aplicar un **modelo de clasificación** para identificar eventos de **alta magnitud**, evaluando su rendimiento con métricas de exactitud y F1-score.  \n",
    "4. Ejecutar un **modelo de clustering (K-Means y DBSCAN)** para segmentar los sismos según sus características espaciales y magnitud, evaluando la calidad de los grupos formados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e265eec",
   "metadata": {},
   "source": [
    "###  Hipótesis\n",
    "\n",
    "- **H1 (Regresión):** La **profundidad** y la **posición geográfica (latitud y longitud)** influyen significativamente en la **magnitud** de los sismos ocurridos en Chile.  \n",
    "- **H2 (Clasificación):** Es posible **predecir correctamente** si un sismo será de **alta magnitud** usando variables de ubicación y profundidad con un rendimiento superior al azar (F1 > 0.6).  \n",
    "- **H3 (Clustering):** Existen **agrupamientos naturales** de sismos que reflejan diferencias geográficas (norte, centro y sur), identificables mediante métricas de calidad como el **coeficiente de silhouette**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42beea27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Entorno OK: 2.3.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, sklearn, matplotlib\n",
    "print(\"✅ Entorno OK:\", pd.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc29bebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando dataset...\n",
      "✓ Dataset cargado exitosamente desde: data\\seismic_data.csv\n",
      "\n",
      "======================================================================\n",
      "INFORMACIÓN BÁSICA DEL DATASET PRUEBA\n",
      "======================================================================\n",
      "Dimensiones: 4,018 filas × 5 columnas\n",
      "Columnas: ['Date(UTC)', 'Latitude', 'Longitude', 'Depth', 'Magnitude']\n",
      "Período: 2012-03-03 11:01:47 a 2025-05-26 03:50:27  (columna: 'Date(UTC)')\n",
      "\n",
      "Primeras filas del dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date(UTC)</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Depth</th>\n",
       "      <th>Magnitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-05-26 03:50:27</td>\n",
       "      <td>-19.63</td>\n",
       "      <td>-69.49</td>\n",
       "      <td>97</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-05-13 00:47:58</td>\n",
       "      <td>-51.25</td>\n",
       "      <td>-72.28</td>\n",
       "      <td>28</td>\n",
       "      <td>5.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-05-05 09:46:48</td>\n",
       "      <td>-29.49</td>\n",
       "      <td>-71.84</td>\n",
       "      <td>48</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-05-05 02:17:48</td>\n",
       "      <td>-31.89</td>\n",
       "      <td>-70.88</td>\n",
       "      <td>88</td>\n",
       "      <td>5.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-05-02 15:23:49</td>\n",
       "      <td>-27.52</td>\n",
       "      <td>-72.48</td>\n",
       "      <td>30</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-05-02 12:58:32</td>\n",
       "      <td>-56.88</td>\n",
       "      <td>-68.06</td>\n",
       "      <td>10</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-04-24 03:20:08</td>\n",
       "      <td>-30.76</td>\n",
       "      <td>-71.90</td>\n",
       "      <td>31</td>\n",
       "      <td>5.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-04-18 00:10:18</td>\n",
       "      <td>-23.47</td>\n",
       "      <td>-68.22</td>\n",
       "      <td>139</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-04-08 22:30:45</td>\n",
       "      <td>-21.05</td>\n",
       "      <td>-68.56</td>\n",
       "      <td>126</td>\n",
       "      <td>5.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-04-08 10:28:43</td>\n",
       "      <td>-17.99</td>\n",
       "      <td>-69.93</td>\n",
       "      <td>143</td>\n",
       "      <td>4.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date(UTC)  Latitude  Longitude  Depth  Magnitude\n",
       "0  2025-05-26 03:50:27    -19.63     -69.49     97        5.6\n",
       "1  2025-05-13 00:47:58    -51.25     -72.28     28        5.1\n",
       "2  2025-05-05 09:46:48    -29.49     -71.84     48        5.0\n",
       "3  2025-05-05 02:17:48    -31.89     -70.88     88        5.1\n",
       "4  2025-05-02 15:23:49    -27.52     -72.48     30        5.8\n",
       "5  2025-05-02 12:58:32    -56.88     -68.06     10        7.5\n",
       "6  2025-04-24 03:20:08    -30.76     -71.90     31        5.4\n",
       "7  2025-04-18 00:10:18    -23.47     -68.22    139        6.0\n",
       "8  2025-04-08 22:30:45    -21.05     -68.56    126        5.2\n",
       "9  2025-04-08 10:28:43    -17.99     -69.93    143        4.8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CARGA Y PREPARACIÓN INICIAL DE DATOS\n",
    "# =============================================================================\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Cargando dataset...\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Configuración general: nombre esperado y estructura recomendada\n",
    "# Estructura esperada del ZIP:\n",
    "# BI_T2_CardenasNicolas/\n",
    "# ├── BI_T2_CardenasNicolas.ipynb\n",
    "# ├── data/\n",
    "# │   └── seismic_data.csv\n",
    "# -------------------------------------------------------------------------\n",
    "NOMBRE_ARCHIVO = \"seismic_data.csv\"\n",
    "RUTAS_CANDIDATAS = [\n",
    "    os.path.join(\"data\", NOMBRE_ARCHIVO),      # 1) ./data/\n",
    "    NOMBRE_ARCHIVO,                            # 2) carpeta actual\n",
    "    os.path.join(\"..\", \"data\", NOMBRE_ARCHIVO) # 3) ../data/\n",
    "]\n",
    "\n",
    "# Buscar el archivo en esas rutas\n",
    "archivo_encontrado = next((p for p in RUTAS_CANDIDATAS if os.path.exists(p)), None)\n",
    "\n",
    "# Búsqueda recursiva adicional (por si cambió el nombre de carpeta)\n",
    "if not archivo_encontrado:\n",
    "    resultados = glob.glob(f\"**/{NOMBRE_ARCHIVO}\", recursive=True)\n",
    "    if resultados:\n",
    "        archivo_encontrado = resultados[0]\n",
    "\n",
    "# Validación final\n",
    "if not archivo_encontrado:\n",
    "    print(f\"\\n❌ ERROR: No se encuentra el archivo '{NOMBRE_ARCHIVO}'.\")\n",
    "    print(\"Asegúrate de incluirlo dentro de la carpeta 'data/' al comprimir el proyecto.\")\n",
    "    print(f\"Directorio actual: {os.getcwd()}\")\n",
    "    print(\"Archivos en esta carpeta:\", os.listdir(\".\"))\n",
    "    raise FileNotFoundError(f\"No se encontró el archivo '{NOMBRE_ARCHIVO}'\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Cargar el CSV con tolerancia a codificación y separador \n",
    "# -------------------------------------------------------------------------\n",
    "def cargar_csv_robusto(ruta):\n",
    "    \"\"\"Intenta leer el CSV usando diferentes configuraciones para evitar errores.\"\"\"\n",
    "    for args in [\n",
    "        {\"sep\": \",\", \"encoding\": \"utf-8\"},\n",
    "        {\"sep\": \";\", \"encoding\": \"utf-8\"},\n",
    "        {\"sep\": \",\", \"encoding\": \"latin-1\"},\n",
    "        {\"sep\": \";\", \"encoding\": \"latin-1\"},\n",
    "    ]:\n",
    "        try:\n",
    "            df = pd.read_csv(ruta, **args)\n",
    "            if len(df.columns) > 1:\n",
    "                return df\n",
    "        except Exception:\n",
    "            continue\n",
    "    raise ValueError(f\"No se pudo leer el archivo CSV '{ruta}' con los encodings probados.\")\n",
    "\n",
    "df = cargar_csv_robusto(archivo_encontrado)\n",
    "\n",
    "print(f\"✓ Dataset cargado exitosamente desde: {archivo_encontrado}\\n\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Información básica del dataset \n",
    "# -------------------------------------------------------------------------\n",
    "print(\"=\" * 70)\n",
    "print(\"INFORMACIÓN BÁSICA DEL DATASET PRUEBA\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Dimensiones: {df.shape[0]:,} filas × {df.shape[1]} columnas\")\n",
    "print(\"Columnas:\", list(df.columns))\n",
    "\n",
    "# Intento de detectar columna de fecha\n",
    "fecha_cols = [c for c in df.columns if any(k in str(c).lower() for k in [\"fecha\", \"date\", \"time\"])]\n",
    "col_fecha = None\n",
    "for c in fecha_cols:\n",
    "    try:\n",
    "        aux = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "        if aux.notna().any():\n",
    "            col_fecha = c\n",
    "            break\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "if col_fecha:\n",
    "    serie_fecha = pd.to_datetime(df[col_fecha], errors=\"coerce\")\n",
    "    print(f\"Período: {serie_fecha.min()} a {serie_fecha.max()}  (columna: '{col_fecha}')\")\n",
    "else:\n",
    "    print(\"Período: no identificado (no se detectó columna de fecha legible)\")\n",
    "\n",
    "print(\"\\nPrimeras filas del dataset:\")\n",
    "display(df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0685fd76",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1f03b9c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
